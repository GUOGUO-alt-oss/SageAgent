#!/usr/bin/env python3
import uuid
from pathlib import Path
import json
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
import shutil
import client as pipeline
from threading import Thread
import time
import llm_analyzer as la

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
PROGRESS = {}
RESULTS = {}

def read_jsonl(path):
    out = []
    p = Path(path)
    if not p.exists():
        return out
    with open(p, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                out.append(json.loads(line))
            except Exception:
                continue
    return out

@app.get("/")
def index():
    return FileResponse(Path("static")/"chat.html")

@app.get("/process_ui")
def process_ui():
    return FileResponse(Path("static")/"process.html")

def _set_progress(uid, stage, steps, error=None):
    PROGRESS[uid] = {"uid": uid, "stage": stage, "steps": steps, "error": error}

def _run_job(uid, save_path, params):
    try:
        base = Path("web_out")/uid
        outdir = base/"out"
        cleanout = base/"cleanout"
        finalout = base/"finalout"
        tmp_audio_dir = base/"tmp_audio"
        pipeline.ensure_dirs(str(outdir), str(cleanout), str(finalout))
        steps = {"upload": "done", "asr": "pending", "clean": "pending", "chapters": "pending", "summaries": "pending", "llm": "pending", "done": "pending"}
        _set_progress(uid, "asr", steps)
        train_jsonl = str(outdir/"train.jsonl")
        pipeline.run_asr(str(save_path), train_jsonl, str(tmp_audio_dir), params["engine"], params["model_size"], params["device"], params["compute_type"], params["language"], params["segment_time"])
        steps["asr"] = "done"
        _set_progress(uid, "clean", steps)
        clean_paragraphs = str(cleanout/"clean_paragraphs.jsonl")
        pipeline.run_clean(train_jsonl, clean_paragraphs, params["min_chars"], params["max_gap_ms"], params["style"])
        steps["clean"] = "done"
        _set_progress(uid, "chapters", steps)
        chapters_jsonl = str(outdir/"chapters.jsonl")
        pipeline.run_chapters(clean_paragraphs, chapters_jsonl, params["min_gap_chapter_ms"], params["min_len_chapter_chars"], params["chapter_threshold"], text_format=True)
        steps["chapters"] = "done"
        _set_progress(uid, "summaries", steps)
        pipeline.run_summaries(train_jsonl, chapters_jsonl, str(finalout), params["window_sec"], bool(params["exam"]), text_format=True)
        steps["summaries"] = "done"
        if params["llm_enable"] and params["llm_api_key"]:
            _set_progress(uid, "llm", steps)
            analysis_path = finalout/"focus_analysis.jsonl"
            try:
                pipeline.run_llm_analysis(clean_paragraphs, str(analysis_path), params["llm_api_key"], params["llm_base_url"], params["llm_model"], text_format=True, dry_run=False)
                steps["llm"] = "done"
            except Exception:
                steps["llm"] = "failed"
        _set_progress(uid, "done", steps)
        micro = read_jsonl(finalout/"micro_summary.jsonl")
        chapter = read_jsonl(finalout/"chapter_summary.jsonl")
        global_ = read_jsonl(finalout/"global_summary.jsonl")
        analysis = []
        ap = finalout/"focus_analysis.jsonl"
        if ap.exists():
            recs = read_jsonl(ap)
            flat = []
            for r in recs:
                items = r.get("items", [])
                for it in items:
                    flat.append(it)
            key_first = []
            non_after = []
            for it in flat:
                cat = str(it.get("ç±»åˆ«", ""))
                typ = str(it.get("type", ""))
                if cat == "é‡ç‚¹" or typ == "key_content":
                    key_first.append(it)
                else:
                    non_after.append(it)
            analysis = key_first + non_after
        RESULTS[uid] = {"uid": uid, "micro_summary": micro, "chapter_summary": chapter, "global_summary": global_, "focus_analysis": analysis}
    except Exception:
        _set_progress(uid, "error", PROGRESS.get(uid, {}).get("steps", {}), error="pipeline_failed")

@app.post("/analyze_clean")
def analyze_clean(
    clean_path: str = Form(...),
    llm_api_key: str = Form(""),
    llm_model: str = Form("deepseek-reasoner"),
    llm_base_url: str = Form("https://api.deepseek.com/v1"),
    dry_run: int = Form(0)
):
    base = Path("web_out")/str(uuid.uuid4())
    finalout = base/"finalout"
    finalout.mkdir(parents=True, exist_ok=True)
    outp = finalout/"focus_analysis.jsonl"
    try:
        pipeline.run_llm_analysis(clean_path, str(outp), llm_api_key, llm_base_url, llm_model, text_format=True, dry_run=bool(dry_run))
    except Exception:
        return JSONResponse(status_code=500, content={"error": "llm_failed"})
    return read_jsonl(outp)

@app.post("/analyze_text")
def analyze_text(
    text: str = Form(...),
    llm_api_key: str = Form(""),
    llm_model: str = Form("deepseek-reasoner"),
    llm_base_url: str = Form("https://api.deepseek.com/v1"),
    dry_run: int = Form(0)
):
    sents = la._split_sentences(text)
    arr = la.analyze_sentences_custom(sents, api_key=llm_api_key, base_url=llm_base_url, model=llm_model, dry_run=bool(dry_run))
    return arr

@app.post("/process")
def process_video(
    video: UploadFile = File(...),
    engine: str = Form("auto"),
    model_size: str = Form("medium"),
    device: str = Form("cuda"),
    compute_type: str = Form("float16"),
    language: str = Form("zh"),
    segment_time: int = Form(120),
    min_chars: int = Form(200),
    max_gap_ms: int = Form(1500),
    style: str = Form("student"),
    min_gap_chapter_ms: int = Form(10000),
    min_len_chapter_chars: int = Form(100),
    chapter_threshold: int = Form(2),
    window_sec: int = Form(60),
    exam: int = Form(0),
    llm_enable: int = Form(1),
    llm_api_key: str = Form("") ,
    llm_model: str = Form("deepseek-reasoner"),
    llm_base_url: str = Form("https://api.deepseek.com/v1")
):
    vid_dir = Path("uploads")
    vid_dir.mkdir(parents=True, exist_ok=True)
    uid = str(uuid.uuid4())
    save_path = vid_dir / f"{uid}_{video.filename}"
    with open(save_path, "wb") as f:
        shutil.copyfileobj(video.file, f)
    base = Path("web_out")/uid
    outdir = base/"out"
    cleanout = base/"cleanout"
    finalout = base/"finalout"
    tmp_audio_dir = base/"tmp_audio"
    pipeline.ensure_dirs(str(outdir), str(cleanout), str(finalout))
    train_jsonl = str(outdir/"train.jsonl")
    try:
        pipeline.run_asr(str(save_path), train_jsonl, str(tmp_audio_dir), engine, model_size, device, compute_type, language, segment_time)
    except Exception:
        return JSONResponse(status_code=500, content={"error": "asr_failed"})
    clean_paragraphs = str(cleanout/"clean_paragraphs.jsonl")
    pipeline.run_clean(train_jsonl, clean_paragraphs, min_chars, max_gap_ms, style)
    chapters_jsonl = str(outdir/"chapters.jsonl")
    pipeline.run_chapters(clean_paragraphs, chapters_jsonl, min_gap_chapter_ms, min_len_chapter_chars, chapter_threshold, text_format=True)
    pipeline.run_summaries(train_jsonl, chapters_jsonl, str(finalout), window_sec, bool(exam), text_format=True)
    analysis = []
    analysis_path = finalout/"focus_analysis.jsonl"
    if llm_enable and llm_api_key:
        try:
            la.analyze_file_custom(clean_paragraphs, str(analysis_path), llm_api_key, llm_base_url, llm_model, dry_run=False)
        except Exception:
            pipeline.run_llm_analysis(clean_paragraphs, str(analysis_path), llm_api_key, llm_base_url, llm_model, text_format=True, dry_run=False)
        recs = read_jsonl(analysis_path)
        flat = []
        for r in recs:
            items = r.get("items", [])
            for it in items:
                flat.append(it)
        key_first = []
        non_after = []
        for it in flat:
            cat = str(it.get("ç±»åˆ«", ""))
            typ = str(it.get("type", ""))
            if cat == "é‡ç‚¹" or typ == "key_content":
                key_first.append(it)
            else:
                non_after.append(it)
        analysis = key_first + non_after
    micro = read_jsonl(finalout/"micro_summary.jsonl")
    chapter = read_jsonl(finalout/"chapter_summary.jsonl")
    global_ = read_jsonl(finalout/"global_summary.jsonl")
    return {"uid": uid, "micro_summary": micro, "chapter_summary": chapter, "global_summary": global_, "focus_analysis": analysis}

@app.post("/start_process")
def start_process(
    video: UploadFile = File(...),
    engine: str = Form("auto"),
    model_size: str = Form("medium"),
    device: str = Form("cuda"),
    compute_type: str = Form("float16"),
    language: str = Form("zh"),
    segment_time: int = Form(120),
    min_chars: int = Form(200),
    max_gap_ms: int = Form(1500),
    style: str = Form("student"),
    min_gap_chapter_ms: int = Form(10000),
    min_len_chapter_chars: int = Form(100),
    chapter_threshold: int = Form(2),
    window_sec: int = Form(60),
    exam: int = Form(0),
    llm_enable: int = Form(1),
    llm_api_key: str = Form(""),
    llm_model: str = Form("deepseek-reasoner"),
    llm_base_url: str = Form("https://api.deepseek.com/v1")
):
    vid_dir = Path("uploads")
    vid_dir.mkdir(parents=True, exist_ok=True)
    uid = str(uuid.uuid4())
    save_path = vid_dir / f"{uid}_{video.filename}"
    with open(save_path, "wb") as f:
        shutil.copyfileobj(video.file, f)
    params = {
        "engine": engine, "model_size": model_size, "device": device, "compute_type": compute_type,
        "language": language, "segment_time": segment_time, "min_chars": min_chars, "max_gap_ms": max_gap_ms,
        "style": style, "min_gap_chapter_ms": min_gap_chapter_ms, "min_len_chapter_chars": min_len_chapter_chars,
        "chapter_threshold": chapter_threshold, "window_sec": window_sec, "exam": exam,
        "llm_enable": llm_enable, "llm_api_key": llm_api_key, "llm_model": llm_model, "llm_base_url": llm_base_url
    }
    _set_progress(uid, "queued", {"upload": "done", "asr": "pending", "clean": "pending", "chapters": "pending", "summaries": "pending", "llm": "pending", "done": "pending"})
    t = Thread(target=_run_job, args=(uid, save_path, params))
    t.daemon = True
    t.start()
    return {"uid": uid}

@app.get("/progress")
def progress(uid: str):
    return PROGRESS.get(uid, {"uid": uid, "stage": "unknown", "steps": {}, "error": "not_found"})

@app.get("/final")
def final(uid: str):
    if uid in RESULTS:
        return RESULTS[uid]
    p = PROGRESS.get(uid)
    if not p:
        return JSONResponse(status_code=404, content={"error": "not_found"})
    if p.get("stage") == "error":
        return JSONResponse(status_code=500, content={"error": p.get("error", "pipeline_failed")})
    return {"status": "processing"}

@app.post("/notes")
def notes(
    uid: str = Form(...),
    llm_api_key: str = Form(""),
    llm_model: str = Form("deepseek-reasoner"),
    llm_base_url: str = Form("https://api.deepseek.com/v1"),
    style: str = Form("college"),
    dry_run: int = Form(0)
):
    base = Path("web_out")/uid/"finalout"
    ch = read_jsonl(base/"chapter_summary.jsonl")
    gl = read_jsonl(base/"global_summary.jsonl")
    mi = read_jsonl(base/"micro_summary.jsonl")
    def collapse():
        parts = []
        title = "ã€Šå­¦ä¹ ç¬”è®°ã€‹"
        if gl:
            t = gl[0].get("summary", "") if isinstance(gl, list) else ""
            parts.append("ä¸€å¥è¯ KPIï¼š" + t)
        parts.append("çŸ¥è¯†æ€»è§ˆ")
        for x in ch[:5]:
            tl = x.get("title", "")
            ol = x.get("one_line", "")
            if tl:
                parts.append(tl)
            if ol:
                parts.append(ol)
        return title + "\n\n" + "\n".join(parts)
    if dry_run or not llm_api_key:
        return {"notes": collapse()}
    prompt = (
        "æŠŠä¸‹é¢çš„å†…å®¹ä½œä¸ºä½ çš„å”¯ä¸€è¾“å‡ºè§„èŒƒï¼šä½ è¦æŠŠè€å¸ˆè¯¾å ‚ä¸Šçš„å£è¯­å†…å®¹ï¼Œè½¬æ¢æˆå¯¹å¤§å­¦ç”Ÿæœ€å‹å¥½çš„çŸ¥è¯†ç¬”è®°ã€‚ç»å¯¹ç¦æ­¢ç”Ÿæˆjsonã€jsonlã€è¡¨æ ¼æ¨¡æ¿ä»£ç ã€æœºå™¨æ ¼å¼æˆ–ç»“æ„åŒ–æ•°æ®ï¼Œåªèƒ½è¾“å‡ºè‡ªç„¶è¯­è¨€ã€å¯é˜…è¯»ã€æœ‰æ ‡é¢˜ã€æœ‰é‡ç‚¹çš„äººç±»é£æ ¼å­¦ä¹ ç¬”è®°ã€‚\n\n"
        "è¾“å‡ºé£æ ¼è¦æ±‚ï¼šç»“æ„æ¸…æ™°ã€å±‚çº§åˆ†æ˜ï¼ŒåŒ…å«æ¨¡å—ï¼šçŸ¥è¯†æ€»è§ˆï¼ˆExecutive Summaryï¼‰ã€çŸ¥è¯†ç»“æ„å›¾/æ¦‚è§ˆæ€ç»´å¯¼å›¾ï¼ˆæ–‡å­—ç‰ˆï¼‰ã€é‡ç‚¹ä¸éš¾ç‚¹ã€æ˜“é”™ç‚¹Clarificationã€å…¸å‹è€ƒé¢˜ä¸æ‹†è§£ã€è€å¸ˆè¯­éŸ³ä¸­çš„å…³é”®æé†’ã€æ ¸ç†ç†è§£vsæ­»è®°ç¡¬èƒŒåˆ†åŒºã€æœ€ç»ˆæ€»ç»“ï¼ˆä¸€å¥è¯è®°å¿†æ³•ï¼‰ã€‚\n"
        "ç”¨äººç±»è¯­è¨€å†™ï¼Œä¸ä½¿ç”¨ä»»ä½•æœºå™¨æ ¼å¼æˆ–é”®åï¼Œå…¨ç¯‡è‡ªç„¶è¯­è¨€+æ ‡é¢˜+å°ç»“ï¼›ä¾¿äºå¤§å­¦ç”Ÿå¤ä¹ ï¼Œé€»è¾‘é“¾æ¡æ˜ç¡®ï¼Œæ¦‚å¿µè§£é‡ŠçŸ­ç‹ å‡†ï¼Œå¯å¸¦æ–‡å­—ç®€å›¾ï¼Œå†…å®¹èƒ½åœ¨5åˆ†é’Ÿå†…å¤ä¹ ä¸€éï¼›èƒ½æç‚¼è€å¸ˆçš„å£è¯­ï¼Œè¯†åˆ«æœ‰ç”¨ä¿¡æ¯ï¼Œåˆ é™¤å£å¤´ç¦…ï¼Œä¿®æ­£å¸¸è¯†æ€§é”™è¯¯ä¸å£è¯¯ï¼Œæç‚¼é€»è¾‘é¡ºåºã€‚\n\n"
        "è¾“å‡ºæ¨¡æ¿ï¼š\n"
        "ğŸŒ¿ ã€Šç« èŠ‚åç§°ã€‹å­¦ä¹ ç¬”è®°\n"
        "1ï¸âƒ£ çŸ¥è¯†æ€»è§ˆï¼ˆExecutive Summaryï¼‰\n"
        "ç”¨3â€“6å¥æ€»ç»“å…¨ç« é‡ç‚¹ã€‚\n"
        "2ï¸âƒ£ çŸ¥è¯†ç»“æ„ï¼ˆæ–‡å­—ç‰ˆæ€ç»´å¯¼å›¾ï¼‰\n"
        "å¤§ç‚¹1\nå°ç‚¹A\nå°ç‚¹B\nå¤§ç‚¹2\nå°ç‚¹A\nå°ç‚¹B\n"
        "3ï¸âƒ£ é‡ç‚¹ä¸éš¾ç‚¹\n"
        "é‡ç‚¹1ï¼šè§£é‡Š\né‡ç‚¹2ï¼šè§£é‡Š\néš¾ç‚¹1ï¼šé€šä¿—è®²è§£\néš¾ç‚¹2ï¼šé€šä¿—è®²è§£\n"
        "4ï¸âƒ£ è€å¸ˆè¯­éŸ³é‡Œçš„å…³é”®æé†’\n"
        "è€å¸ˆç‰¹åˆ«å¼ºè°ƒäº†â€¦â€¦\nè€å¸ˆåå¤è¯´çš„é‡ç‚¹æ˜¯â€¦â€¦\n"
        "5ï¸âƒ£ æ˜“é”™ç‚¹ï¼ˆçº æ­£å¸¸è§è¯¯è§£ï¼‰\n"
        "æ˜“é”™ç‚¹1ï¼šæ­£ç¡®è§£é‡Š\næ˜“é”™ç‚¹2ï¼šæ­£ç¡®è§£é‡Š\n"
        "6ï¸âƒ£ å…¸å‹é¢˜å‹æ‹†è§£\n"
        "ä¾‹é¢˜ï¼šï¼ˆé¢˜ç›®é‡è¿°ï¼‰\næ­£ç¡®æ€è·¯ï¼š\né™·é˜±ï¼š\nä¸ºä»€ä¹ˆé”™ï¼š\n"
        "7ï¸âƒ£ ä¸€å¥è¯è®°å¿†æ³•\n"
        "ä¸€å¥è¯—æ€§è®°å¿†å¥ï¼Œè®©æ¦‚å¿µæ°¸ä¸å¿˜ã€‚\n"
        "8ï¸âƒ£ æœ¬ç« å¤ä¹ Checklist\n"
        "æ˜¯å¦ç†è§£â€¦â€¦\næ˜¯å¦èƒ½ç”»å‡ºâ€¦â€¦\næ˜¯å¦èƒ½è§£é‡Šâ€¦â€¦\n"
        "9ï¸âƒ£ ç»“å°¾ï¼ˆæŠ’æƒ…æ”¶å°¾ï¼‰\n"
        "ç”¨ä¸€å¥è¯æŠŠå­¦ä¹ å’Œäººç”Ÿè¿èµ·æ¥ï¼Œè®©ç¬”è®°æœ‰çµé­‚ã€‚\n\n"
        "è¯·æ ¹æ®å…¨å±€æ‘˜è¦ã€ç« èŠ‚æ‘˜è¦ä¸å¾®æ®µæ‘˜è¦ï¼Œç”Ÿæˆä»¥ä¸Šæ ¼å¼çš„å­¦ä¹ ç¬”è®°ï¼Œä»…è¾“å‡ºè‡ªç„¶è¯­è¨€ã€‚"
    )
    src = {
        "global": gl,
        "chapters": ch,
        "micro": mi
    }
    user = json.dumps(src, ensure_ascii=False)
    try:
        url = llm_base_url.rstrip("/") + "/chat/completions"
        headers = {"Authorization": f"Bearer {llm_api_key}", "Content-Type": "application/json"}
        payload = {"model": llm_model, "messages": [{"role":"system","content": prompt},{"role":"user","content": user}], "temperature": 0.2}
        body = la._http_post(url, headers, payload)
        obj = json.loads(body)
        content = obj.get("choices", [{}])[0].get("message", {}).get("content", "")
        return {"notes": content}
    except Exception:
        return {"notes": collapse()}

@app.post("/chat")
def chat(
    prompt: str = Form(...),
    history: str = Form(""),
    llm_api_key: str = Form(""),
    llm_model: str = Form("deepseek-chat"),
    llm_base_url: str = Form("https://api.deepseek.com/v1"),
    system_prompt: str = Form("ä½ æ˜¯ä¸€åä¸“ä¸šçš„åŠ©æ•™ï¼Œå›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€‚")
):
    msgs = []
    if history:
        try:
            arr = json.loads(history)
            for m in arr:
                r = str(m.get("role", "")).strip()
                c = str(m.get("content", ""))
                if r in ("user", "assistant") and c:
                    msgs.append({"role": r, "content": c})
        except Exception:
            pass
    msgs.insert(0, {"role": "system", "content": system_prompt})
    msgs.append({"role": "user", "content": prompt})
    try:
        url = llm_base_url.rstrip("/") + "/chat/completions"
        headers = {"Authorization": f"Bearer {llm_api_key}", "Content-Type": "application/json"}
        payload = {"model": llm_model, "messages": msgs, "temperature": 0.2}
        body = la._http_post(url, headers, payload)
        obj = json.loads(body)
        content = obj.get("choices", [{}])[0].get("message", {}).get("content", "")
        return {"reply": content}
    except Exception:
        return {"reply": "å½“å‰æ— æ³•è¿æ¥åˆ°æ¨¡å‹ï¼Œè¯·ç¨åé‡è¯•ã€‚"}

if __name__ == "__main__":
    try:
        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=8000)
    except Exception:
        print("uvicorn not installed")
