import json
import re
import time
from pathlib import Path
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError

# ==========================
# 1. æ›´å¼º SYSTEM PROMPT
# ==========================
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€åä¸“ä¸šæ•™è‚²å†…å®¹åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯â€œé€å¥åˆ†æè€å¸ˆçš„è®²è¯¾å†…å®¹â€ï¼Œå¹¶ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ã€‚

ã€å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„èŒƒã€‘

æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ å¿…é¡»åŒ…å«å­—æ®µï¼š

1. "å¥å­": åŸå§‹å¥å­
2. "ç±»åˆ«": "é‡ç‚¹" æˆ– "éé‡ç‚¹"
3. è‹¥ä¸ºé‡ç‚¹ï¼š
    - "æ€»ç»“": 1-3 å¥ä¸­æ–‡æ€»ç»“
    - "é‡è¦æ€§è¯´æ˜": ä¸ºä»€ä¹ˆæ˜¯é‡ç‚¹ï¼ˆè€ƒè¯•ç‚¹/æ ¸å¿ƒæ¦‚å¿µ/é«˜é¢‘è€ƒç‚¹ï¼‰
    - "éé‡ç‚¹åŸå› ": å¿…é¡»ä¸ºç©ºå­—ç¬¦ä¸²
4. è‹¥ä¸ºéé‡ç‚¹ï¼š
    - "æ€»ç»“": å¿…é¡»ä¸ºç©ºå­—ç¬¦ä¸²
    - "é‡è¦æ€§è¯´æ˜": å¿…é¡»ä¸ºç©ºå­—ç¬¦ä¸²
    - "éé‡ç‚¹åŸå› ": ä¸ºä»€ä¹ˆä¸é‡è¦ï¼ˆé—²èŠ/ä¸¾ä¾‹/éè€ƒè¯•èŒƒå›´/è¿‡æ¸¡è¯­å¥ç­‰ï¼‰

ã€å¿…é¡»éµå®ˆã€‘
- åªèƒ½è¾“å‡º JSON æ•°ç»„
- ç¦æ­¢å‡ºç°è§£é‡Šã€é¢å¤–æ–‡æœ¬ã€Markdownã€ä»£ç å—
"""



# ==========================
# 2. åˆ†å¥å™¨ï¼ˆä¿æŒä½ åŸç»“æ„ï¼‰
# ==========================
def _split_sentences(s):
    parts = []
    buf = []
    for ch in s:
        buf.append(ch)
        if ch in "ã€‚ï¼ï¼Ÿ.!?":
            part = "".join(buf).strip()
            if part:
                parts.append(part)
            buf = []
    if buf:
        tail = "".join(buf).strip()
        if tail:
            if not re.search(r"[ã€‚ï¼ï¼Ÿ.!?]$", tail):
                if re.search(r"[å—å‘¢å§å•Šå˜›]$", tail):
                    tail += "ï¼Ÿ"
                else:
                    tail += "ã€‚"
            parts.append(tail)
    return parts



# ==========================
# 3. ç»Ÿä¸€ç”¨æˆ· promptï¼šæ›´ç»“æ„åŒ–
# ==========================
def _build_user_prompt(sentences):
    obj = {
        "ä»»åŠ¡æè¿°": "è¯·é€å¥åˆ†æè¿™äº›æ•™å¸ˆè®²è¯¾å†…å®¹ã€‚",
        "å¥å­åˆ—è¡¨": sentences,
        "è¾“å‡ºè¦æ±‚": "ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”è¾“å…¥å¥å­åºå·ï¼Œä¸å¾—æ”¹å˜é¡ºåºã€‚"
    }
    return json.dumps(obj, ensure_ascii=False)

PROMPT_CLASSROOM = (
    "ä½ æ˜¯ä¸€åè®­ç»ƒæœ‰ç´ çš„æ•™è‚²å†…å®¹åˆ†æå¼•æ“ã€‚" 
    "è¾“å…¥æ˜¯ä¸€æ®µè€å¸ˆçš„çœŸå®æˆè¯¾å†…å®¹ï¼Œè¯·ä½ è¿›è¡Œç»“æ„åŒ–å†…å®¹åˆ†æï¼Œè¾“å‡º JSONã€‚"
    "\nä½ çš„ä»»åŠ¡ï¼š\n"
    "è¯†åˆ«é‡ç‚¹å†…å®¹ï¼šåŒ…å«å…³é”®å®šç†ã€ç»“è®ºã€æ˜“é”™ç‚¹ï¼Œæˆ–è€å¸ˆå¼ºè°ƒâ€˜é‡è¦â€™â€˜è€ƒè¯•ä¼šè€ƒâ€™â€˜å¿…é¡»è®°ä½â€™ â†’ ç”¨ type: key_content\n"
    "è¯†åˆ«æ¬¡è¦å†…å®¹ï¼ˆå¯ç•¥è¿‡ã€ä¸é‡è¦ï¼‰ï¼šå¦‚â€˜è¿™è¯¾ä¸è€ƒâ€™â€˜ç®€å•â€™â€˜éšä¾¿çœ‹çœ‹â€™â€˜è·³è¿‡ä¹Ÿè¡Œâ€™ â†’ ç”¨ type: minor_content\n"
    "è¯†åˆ«åŸºç¡€å®šä¹‰/æ¦‚å¿µï¼šç”¨ type: definition\n"
    "è¯†åˆ«ä¾‹é¢˜è®²è§£ï¼šç”¨ type: example\n"
    "æ£€æµ‹è€å¸ˆçš„æ•™å­¦æ„å›¾/å…ƒä¿¡æ¯ï¼ˆæé†’ã€æ€»ç»“ã€è½¬åœºï¼‰ï¼šç”¨ type: meta\n"
    "\næ¯å¥è¯éƒ½è¦è¾“å‡ºä¸€å¥åˆ†æï¼Œä¸è¦é—æ¼ã€‚åªè¾“å‡º JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« text ä¸ typeã€‚"
)

def _build_user_prompt_lines(sentences):
    return "\n".join(f"{i+1}. {s}" for i, s in enumerate(sentences))

def _heuristic_type(s):
    s2 = str(s)
    if any(w in s2 for w in ["å®šä¹‰", "æ¦‚å¿µ", "ç§°ä¸º", "æ˜¯æŒ‡"]):
        return "definition"
    if any(w in s2 for w in ["ä¾‹é¢˜", "ä¾‹å­", "ä¾‹å¦‚", "ä¸¾ä¾‹", "æ¯”å¦‚"]):
        return "example"
    if any(w in s2 for w in ["æ€»ç»“", "æé†’", "æ³¨æ„", "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å", "è½¬åœº", "æ¥ç€", "æ‰€ä»¥"]):
        return "meta"
    if any(w in s2 for w in ["ä¸è€ƒ", "ç®€å•", "éšä¾¿çœ‹çœ‹", "è·³è¿‡", "ç•¥è¿‡"]):
        return "minor_content"
    if any(w in s2 for w in ["å®šç†", "ç»“è®º", "æ˜“é”™ç‚¹", "é‡è¦", "è€ƒè¯•ä¼šè€ƒ", "å¿…é¡»è®°ä½"]):
        return "key_content"
    return "meta"



# ==========================
# 4. HTTP è¯·æ±‚
# ==========================
def _http_post(url, headers, payload, timeout=60):
    data = json.dumps(payload).encode("utf-8")
    req = Request(url, data=data, headers=headers, method="POST")
    with urlopen(req, timeout=timeout) as resp:
        return resp.read().decode("utf-8")


def _call_deepseek(system_prompt, user_prompt, api_key, base_url, model):
    url = base_url.rstrip("/") + "/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "temperature": 0.2,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    }
    try:
        body = _http_post(url, headers, payload)
        obj = json.loads(body)
        content = obj.get("choices", [{}])[0].get("message", {}).get("content", "")
        return content
    except Exception:
        return ""



# ==========================
# 5. JSON æå–å™¨ï¼ˆæ›´èªæ˜ï¼‰
# ==========================
def _extract_json_array(text):
    if not text:
        return None

    # å…ˆæ¸…ç†ï¼šâ€œ```json ... ```â€ ç±»æ ¼å¼
    text = text.strip()
    text = re.sub(r"^```json", "", text)
    text = re.sub(r"```$", "", text)
    text = text.strip()

    # ç›´æ¥è§£æ
    if text.startswith("["):
        try:
            return json.loads(text)
        except:
            pass

    # ä»ä»»æ„ä½ç½®æå–
    m1 = text.find("[")
    m2 = text.rfind("]")
    if m1 != -1 and m2 != -1 and m2 > m1:
        try:
            return json.loads(text[m1:m2+1])
        except:
            pass

    return None



# ==========================
# 6. æœ¬åœ°å…œåº•è§„åˆ™ï¼ˆLLM å¤±è´¥æ—¶ç”¨ï¼‰
# ==========================
def _heuristic_item(s):
    kw = ["å®šä¹‰", "å®šç†", "æ€§è´¨", "å…¬å¼", "è¯æ˜", "æ¦‚å¿µ", "ç»“è®º", "æ³¨æ„", "é‡ç‚¹", "è€ƒè¯•"]
    for w in kw:
        if w in s:
            return {
                "å¥å­": s,
                "ç±»åˆ«": "é‡ç‚¹",
                "æ€»ç»“": s[:50],
                "é‡è¦æ€§è¯´æ˜": "æ¶‰åŠè€ƒè¯•æˆ–æ ¸å¿ƒæ¦‚å¿µ",
                "éé‡ç‚¹åŸå› ": ""
            }
    return {
        "å¥å­": s,
        "ç±»åˆ«": "éé‡ç‚¹",
        "æ€»ç»“": "",
        "é‡è¦æ€§è¯´æ˜": "",
        "éé‡ç‚¹åŸå› ": "ä¸¾ä¾‹/è¿‡æ¸¡/éè€ƒè¯•èŒƒå›´"
    }



# ==========================
# 7. ä¸»å¥å­åˆ†æå™¨
# ==========================
def analyze_sentences(sentences, api_key, base_url, model, dry_run=False):
    if not sentences:
        return []

    if dry_run:
        return [_heuristic_item(s) for s in sentences]

    user_prompt = _build_user_prompt(sentences)
    content = _call_deepseek(SYSTEM_PROMPT, user_prompt, api_key, base_url, model)

    arr = _extract_json_array(content)

    # è‹¥ LLM æˆåŠŸï¼Œç¡®ä¿ index å¯¹é½
    if isinstance(arr, list) and len(arr) == len(sentences):
        return arr

    # LLM å¤±è´¥ â†’ å¯ç”¨å…œåº•
    return [_heuristic_item(s) for s in sentences]

def analyze_sentences_custom(sentences, api_key, base_url, model, dry_run=False):
    if not sentences:
        return []
    if dry_run:
        return [{"text": s, "type": _heuristic_type(s)} for s in sentences]
    user_prompt = _build_user_prompt_lines(sentences) + "\nè¯·æŒ‰æ¯å¥è¾“å‡ºä¸€ä¸ªå¯¹è±¡ï¼Œå­—æ®µåŒ…å« text ä¸ typeï¼Œåªè¾“å‡º JSON æ•°ç»„ã€‚"
    content = _call_deepseek(PROMPT_CLASSROOM, user_prompt, api_key, base_url, model)
    arr = _extract_json_array(content)
    if isinstance(arr, list):
        return arr
    return [{"text": s, "type": _heuristic_type(s)} for s in sentences]



# ==========================
# 8. æŒ‰æ®µè½åˆ†æ â†’ å†™å…¥ jsonl
# ==========================
def analyze_file(clean_paragraphs_path, output_jsonl_path, api_key, base_url, model, dry_run=False):
    outp = Path(output_jsonl_path)
    outp.parent.mkdir(parents=True, exist_ok=True)

    with open(clean_paragraphs_path, "r", encoding="utf-8") as f_in, open(outp, "w", encoding="utf-8") as f_out:
        for line in f_in:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except:
                continue

            text = str(obj.get("text", ""))
            sentences = _split_sentences(text)
            items = analyze_sentences(sentences, api_key, base_url, model, dry_run=dry_run)

            rec = {
                "video_id": obj.get("video_id"),
                "paragraph_id": obj.get("paragraph_id"),
                "start_ms": obj.get("start_ms"),
                "end_ms": obj.get("end_ms"),
                "items": items
            }
            f_out.write(json.dumps(rec, ensure_ascii=False) + "\n")

def analyze_file_custom(clean_paragraphs_path, output_jsonl_path, api_key, base_url, model, dry_run=False):
    outp = Path(output_jsonl_path)
    outp.parent.mkdir(parents=True, exist_ok=True)
    with open(clean_paragraphs_path, "r", encoding="utf-8") as f_in, open(outp, "w", encoding="utf-8") as f_out:
        for line in f_in:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except:
                continue
            text = str(obj.get("text", ""))
            sentences = _split_sentences(text)
            items = analyze_sentences_custom(sentences, api_key, base_url, model, dry_run=dry_run)
            rec = {
                "video_id": obj.get("video_id"),
                "paragraph_id": obj.get("paragraph_id"),
                "start_ms": obj.get("start_ms"),
                "end_ms": obj.get("end_ms"),
                "items": items
            }
            f_out.write(json.dumps(rec, ensure_ascii=False) + "\n")



def write_analysis_text(analysis_jsonl_path, text_output_path):
    """å°†LLMåˆ†æç»“æœå†™å…¥æ˜“è¯»çš„æ–‡æœ¬æ ¼å¼"""
    Path(text_output_path).parent.mkdir(parents=True, exist_ok=True)
    
    key_sentences = []
    non_key_sentences = []
    total_sentences = 0
    
    with open(analysis_jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
                items = obj.get("items", [])
                
                for item in items:
                    total_sentences += 1
                    category = item.get("ç±»åˆ«", "éé‡ç‚¹")
                    sentence = item.get("å¥å­", "")
                    
                    if category == "é‡ç‚¹":
                        key_sentences.append({
                            "sentence": sentence,
                            "summary": item.get("æ€»ç»“", ""),
                            "importance": item.get("é‡è¦æ€§è¯´æ˜", ""),
                            "video_id": obj.get("video_id"),
                            "start_ms": obj.get("start_ms")
                        })
                    else:
                        non_key_sentences.append({
                            "sentence": sentence,
                            "reason": item.get("éé‡ç‚¹åŸå› ", ""),
                            "video_id": obj.get("video_id"),
                            "start_ms": obj.get("start_ms")
                        })
            except Exception:
                continue
    
    with open(text_output_path, "w", encoding="utf-8") as f:
        f.write("=" * 60 + "\n")
        f.write("ğŸ¯ LLMé‡ç‚¹åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        key_count = len(key_sentences)
        non_key_count = len(non_key_sentences)
        key_percentage = (key_count / total_sentences * 100) if total_sentences > 0 else 0
        
        f.write("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š\n")
        f.write(f"   â€¢ æ€»å¥å­æ•°ï¼š{total_sentences}\n")
        f.write(f"   â€¢ é‡ç‚¹å¥å­ï¼š{key_count} ({key_percentage:.1f}%)\n")
        f.write(f"   â€¢ éé‡ç‚¹å¥å­ï¼š{non_key_count} ({100-key_percentage:.1f}%)\n\n")
        
        # é‡ç‚¹å†…å®¹
        if key_sentences:
            f.write("ğŸ”¥ é‡ç‚¹å†…å®¹åˆ†æï¼š\n")
            f.write("=" * 50 + "\n\n")
            
            for i, item in enumerate(key_sentences, 1):
                start_ms = item.get("start_ms", 0)
                start_min = start_ms // 60000
                start_sec = (start_ms % 60000) // 1000
                
                f.write(f"ã€é‡ç‚¹ {i:02d}ã€‘ {start_min:02d}:{start_sec:02d}\n")
                f.write(f"ğŸ“ åŸå¥ï¼š{item['sentence']}\n")
                f.write(f"ğŸ’ æ€»ç»“ï¼š{item['summary']}\n")
                f.write(f"â­ é‡è¦æ€§ï¼š{item['importance']}\n")
                f.write("-" * 40 + "\n\n")
        
        # éé‡ç‚¹å†…å®¹ï¼ˆä»…æ˜¾ç¤ºå‰10ä¸ªä½œä¸ºç¤ºä¾‹ï¼‰
        if non_key_sentences:
            f.write("ğŸ“ éé‡ç‚¹å†…å®¹ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰ï¼š\n")
            f.write("=" * 50 + "\n\n")
            
            for i, item in enumerate(non_key_sentences[:10], 1):
                start_ms = item.get("start_ms", 0)
                start_min = start_ms // 60000
                start_sec = (start_ms % 60000) // 1000
                
                f.write(f"ã€éé‡ç‚¹ {i:02d}ã€‘ {start_min:02d}:{start_sec:02d}\n")
                f.write(f"ğŸ“ åŸå¥ï¼š{item['sentence']}\n")
                f.write(f"ğŸ“„ åŸå› ï¼š{item['reason']}\n")
                f.write("-" * 30 + "\n\n")
            
            if len(non_key_sentences) > 10:
                f.write(f"... è¿˜æœ‰ {len(non_key_sentences) - 10} ä¸ªéé‡ç‚¹å¥å­æœªæ˜¾ç¤º\n\n")
        
        f.write("âœ… åˆ†æå®Œæˆï¼å»ºè®®é‡ç‚¹å…³æ³¨æ ‡ä¸ºğŸ”¥çš„å†…å®¹ã€‚\n")

# ==========================
# 9. å¯¹å¤–æ¥å£
# ==========================
def run_llm_analysis(clean_path, out_path, api_key, base_url, model, dry_run=False):
    analyze_file(clean_path, out_path, api_key, base_url, model, dry_run)
